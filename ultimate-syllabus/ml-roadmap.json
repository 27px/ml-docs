{
    "Machine Learning": {
        "Process": {
            "Data Collection": ["what kind of problem are we trying to solve", "existing datasets", "what primary concerns are there", "is the data public"],
            "Data Preperation": {
                "EDA": ["features (input) and variables (output)", "type of data", "missing values", "outliers", "domain expert"],
                "Data Processing": {
                    "Feature Imputation: filling missing values": {
                        "Single Imputation": "fill with ean, median or column",
                        "Multiple Imputation": "model with other values and fill with model output",
                        "KNN (k-nearest-neighbour)": "fill data with a value from another example which is similar",
                        "Others": "random, last observation (carry forward, in time series), moving window, most frequent"
                    },
                    "Feature Encoding, turning vales into numbers": ["OneHotEncoding", "LabelEncoding", "EmbeddingEncoding"],
                    "Feature Normalization": {
                        "Feature Scaling": "also called normalization, transforms data between 0 and 1",
                        "Feature Standardization": "standardizes values so that they have a mean of 0 and unit variance"
                    },
                    "Feature Engineering, transform data into more meaningful representation by adding domain knowledge": {
                        "Decompose": "timestamp to day month ..., day_of_week etc",
                        "Discretization": "turning large groups into smaller ones, eg: age into above_50, under_40 etc, this process is known as binning, also different types of green like dark green, light green into green category, etc",
                        "Crossing & Interaction features": "combining two or more features, eg: house_last_sold_date and current_date to get time_on_market",
                        "Indicator Features": "X_is_missing column as indicator"
                    },
                    "Feature Selection": "selecting features that are most valuable, reduces overfitting, reduces training time, processing, and improving accuracy. Dimensionality reduction, PCA. Feature Importance, remove least important ones",
                    "Dealing with imbalances": ["collect more data if possible", "https://imbalanced-learn.org/stable/", "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/", "https://web.uri.edu/cisa/"]
                },
                "Data Splitting": "train:test:validation usually is 70:15:15 or 80:10:10"
            },
            "Train Model on data": {
                "Choosing an Algorithm": {
                    "Supervised": {
                        "Linear Regression": null,
                        "Logistic Regression": null,
                        "k-Nearst Neighbour": null,
                        "SVM": null,
                        "Decision Tree": null,
                        "Random Forest": null,
                        "Ada Boost/Gradient Boost": ["XGBoost", "CatBoost", "LightGBM"],
                        "Neural Networks": null
                    },
                    "Unsupervised": {
                        "Clustering": null,
                        "Visualization and Dimensionality Reduction": ["PCA", "t-SNE", "AutoEncoder"],
                        "Anomaly Detection": ["AutoEncoder", "One-Class Classification"]
                    }
                },
                "Types of learning": ["Batch Learning", "Online Learning", "Transfer Learning", "Active Learning", "Ensembling"],
                "Underfitting": "model underperforms, try learning longer, or use advanced models",
                "Overfitting": {
                    "regularization": ["L1 (Lasso)", "L2 (Ridge)", "Dropout", "Early Stopping", "Data Augmentation", "Batch Normalization"]
                },
                "Hyperparameter Tuning": ["learning rate: often most important, high lr will cause algorithm to raidly adapt to new data (but might not be useful accuracy), lower ones slow adaptation (but accurate), (eg for transfer learning)", "https://arxiv.org/abs/1803.09820", "other params to tune: number of layers (NN), batch size, number of trees (rf), iterations"]
            },
            "Analysis/Evaluation": ["Evaluation metrics", "Feature Importance, which feature contributes most to the model? should some be removed etc.", "how long does model training take, is it feasible", "how long does inference take, is it suitable for production", "least confident examples, what does the model get wrong", "bias/variance trade-off"],
            "Deployment": {
                "MLOps": "https://huyenchip.com/2020/06/22/mlops.html",
                "TensorFlow": "https://www.tensorflow.org/tfx/guide/serving",
                "Pytorch": "https://pytorch.org/serve/",
                "Google AI Platform": "https://cloud.google.com/vertex-ai",
                "Sagemaker": "https://aws.amazon.com/sagemaker/"
            },
            "Retrain Model": "continuously evaluate how the model performs after deployment, models will 'age' in time, degrade in performance due to 'drift' and seasonal changes etc"
        },
        "Resources": {
            "Code": [
                "https://towardsdatascience.com/how-docker-can-help-you-become-a-more-effective-data-scientist-7fc048ef91d5",
                "https://missing.csail.mit.edu/",
                "https://teachyourselfcs.com/",
                "https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-3-structured-data-projects/end-to-end-heart-disease-classification.ipynb",
                "https://github.com/dformoso/sklearn-classification",
                "https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb",
                "https://www.fast.ai/",
                "https://course.fast.ai/",
                "https://www.deeplearning.ai/",
                "https://cs50.harvard.edu/ai/2020/",
                "https://fullstackdeeplearning.com/march2019",
                "https://www.youtube.com/watch?v=rfscVS0vtbw",
                "https://zerotomastery.io/",
                "https://www.pythonlikeyoumeanit.com/",
                "https://www.mrdbourke.com/get-your-computer-ready-for-machine-learning-using-anaconda-miniconda-and-conda/",
                "https://realpython.com/python-virtual-environments-a-primer/",
                "https://jupyter-notebook.readthedocs.io/en/stable/",
                "https://www.mrdbourke.com/mlcourse/",
                "https://www.kaggle.com/learn",
                "https://www.datacamp.com/",
                "https://www.dataquest.io/"
            ],
            "Books": [
                "Automate the Boring Stuff with Python, 2nd Edition: Practical Programming for Total Beginners 2nd Edition",
                "Python-Data-Analysis-Wrangling-IPython",
                "https://jakevdp.github.io/PythonDataScienceHandbook/",
                "Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems",
                "Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD",
                "http://neuralnetworksanddeeplearning.com/index.html",
                "Introduction to Machine Learning with Python: A Guide for Data Scientists Paperback",
                "https://mlbook.explained.ai/",
                "https://www.buildingmlpipelines.com/",
                "https://christophm.github.io/interpretable-ml-book/intro.html",
                "https://seeing-theory.brown.edu/index.html#firstPage",
                "https://mml-book.github.io/",
                "https://explained.ai/matrix-calculus/index.html"
            ],
            "Concepts and process": ["https://www.elementsofai.com/", "https://developers.google.com/machine-learning/crash-course", "https://ai.google/build", "https://research.facebook.com/blog/2018/05/the-facebook-field-guide-to-machine-learning-video-series/", "https://madewithml.com/topics/", "https://workera.ai/", "https://www.kaggle.com/competitions", "https://karpathy.medium.com/software-2-0-a64152b37c35"],
            "Mathematics": ["https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY", "https://www.khanacademy.org/math/multivariable-calculus", "https://www.3blue1brown.com/topics/linear-algebra", "https://www.khanacademy.org/math/calculus-1", "https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices", "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi", "https://www.khanacademy.org/math/statistics-probability"],
            "Rules and Tidbits": ["https://fastpages.fast.ai/", "https://pages.github.com/", "https://karpathy.github.io/2019/04/25/recipe/", "https://developers.google.com/machine-learning/guides/rules-of-ml/"],
            "Datasets": ["https://datasetsearch.research.google.com/", "https://www.kaggle.com/datasets", "https://www.dataquest.io/blog/free-datasets-for-projects/", "https://storage.googleapis.com/openimages/web/index.html", "https://github.com/HumanSignal/awesome-data-labeling", "https://index.quantumstat.com/"],
            "Papers": ["https://arxiv-sanity-lite.com/", "https://madewithml.com/", "https://paperswithcode.com/", "https://sotabench.com/"],
            "Others": ["https://www.coursera.org/learn/learning-how-to-learn/", "https://www.mrdbourke.com/6-techniques-which-help-me-study-machine-learning-five-days-per-week/", "https://www.mrdbourke.com/aimastersdegree/"],
            "Cloud Services": ["https://www.pluralsight.com/cloud-guru", "https://cloud.google.com/learn/training", "https://aws.amazon.com/training/", "https://learn.microsoft.com/en-us/training/azure/"]
        },
        "Problems": {
            "Categories": {
                "Supervised": "labelled data",
                "Unsupervised": "finding patterns in unlabelled data",
                "Reinforcement Learning": "training using reinforcement",
                "Transfer Learning": "transferring knowledge"
            },
            "Regression": {
                "Example Problem": "stock price prediction, linear predictions",
                "Evaluation Metrics": {
                    "R-squared error": "perfect model scores 1.0",
                    "MSE (Mean Squared Error": "makes outliers stand out more, negative won't cancel positive, being 10% off is way higher than double of being 5% off",
                    "MAE (Mean Absolute Error)": "All errors on same scale, predicting 99 instead of 100 is same as predicting 101 instead of 100",
                    "RMSE (Rooted Mean Squared Error)": "same as MAE, rooted so gets in the same unit"
                }
            },
            "Dimensionality Reduction": "reducing number of features without loosing much meaning",
            "Clustering": "grouping peronas based on activity",
            "Sequence to Sequence (seq2seq)": "example nlp translation",
            "Classification": {
                "Problems": {
                    "Binary Classification": "cat or dog (true/false)",
                    "Multi-class Classification": "red, blue or green (multiple choises)",
                    "Multi-label Classification": "items in a photo (multiple, tags, keywords etc)"
                },
                "Evaluation Metrics": {
                    "Confusion Matrix": "useful for seeing where a model is getting confused",
                    "Accuracy": "out of 100 how many got it right, unreliable when class count is unbalanced, in that case use precision and recall instead",
                    "F1 Score": "The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0, f1_score = 2 * (precision * recall) / (precision + recall)",
                    "Precision": "accuracy of the positive predictions, a model with 1.0 precision score has no false positives",
                    "Recall": "ration of true positive predictions, a model with 1.0 recall has no false negatives",
                    "Precision Recall Tradeoff": "Increasing precision reduces recall, and vice versa, optimizing for one will reduce the other",
                    "Precision Recall Curve": "Plotting precision vs recall at different classification thresholds allow you to choose a precision value at a given recall value (as one increases, the other one decreases)",
                    "ROC Curve / AOC": "The receiver operating characteristic (ROC) curve is a common way to evaluate binary classifiers, Plots false positive rate against true positive rate, AUC (Area under the curve) 1.0 is perfect classifier, generally if don't have much positive examples or you care about false positive than false negative then use precision/recall curve rather than ROC/AUC curve",
                    "mean average precision": "used for evaluating object detection tasks and information retrieval"
                }
            }
        },
        "Tools": {
            "Toolbox": {
                "pretrained models": {
                    "Tensorflow Hub": "https://www.tensorflow.org/hub/",
                    "Pytorch Hub": "https://pytorch.org/hub/",
                    "HuggingFace Transformers (NLP)": "https://huggingface.co/docs/transformers/index",
                    "Detectron2 (CV)": "https://github.com/facebookresearch/detectron2"
                },
                "experiment tracking": {
                    "Tensorboard": "https://www.tensorflow.org/tensorboard/",
                    "Dashboard by weights and biases": "https://wandb.ai/site/experiment-tracking",
                    "neptune.ai": "https://neptune.ai/"
                },
                "Data & Model tracking": {
                    "Artifacts by weights and biases": "https://wandb.ai/site/artifacts",
                    "DVC": "https://dvc.org/"
                },
                "Cloud Services": {
                    "Google Colab": "https://colab.research.google.com/",
                    "AWS": "https://aws.amazon.com/machine-learning/",
                    "AWS Sagemaker": "https://aws.amazon.com/sagemaker/",
                    "GCP AI Platform": "https://cloud.google.com/vertex-ai",
                    "Azure ML": "https://azure.microsoft.com/en-us/products/machine-learning/"
                },
                "Hardware, building": {
                    "Choosing GPU": "https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/",
                    "Building a PC for DL": "https://medium.com/the-mission/how-to-build-the-perfect-deep-learning-computer-and-save-thousands-of-dollars-9ec3b2eb4ce2"
                },
                "Auto ML": {
                    "TPot": "https://github.com/EpistasisLab/tpot",
                    "Google Cloud AutoML": "https://cloud.google.com/automl/",
                    "Microsoft automated ML": "https://azure.microsoft.com/en-us/products/machine-learning/automatedml/",
                    "Sweeps by weights and biases": "https://wandb.ai/site/sweeps",
                    "Keras Tuner": "https://www.tensorflow.org/tutorials/keras/keras_tuner"
                },
                "Explainability (Why did my model do what it did)": {
                    "What-If tool": "https://pair-code.github.io/what-if-tool/",
                    "SHAP values": "https://github.com/shap/shap"
                },
                "Machine Learning Lifecycle": {
                    "Streamlit": "https://streamlit.io/",
                    "MLflow": "https://mlflow.org/",
                    "Kubeflow": "https://www.kubeflow.org/",
                    "Seldon": "https://www.seldon.io/",
                    "Production LevelDL Example Repo": "https://github.com/alirezadir/Production-Level-Deep-Learning"
                }
            },
            "Libraries": ["scikit-learn", "pytorch", "pytorch lightning", "tensorflow", "keras", "ONNX"]
        },
        "Mathematics": {
            "Linear Algebra": "creating objects and set of rules to manipulate these objects",
            "Matrix Manipulation": "ml data is eventually turned into matrices and tensors",
            "Multivariate Calculus": "foundation for optimizing function like cost functions, with respect to multiple parameters",
            "Probability Distribution": "probabilities, sample space, series of possible events, random events",
            "Pobability": "study of uncertainity",
            "Optimization": "finding the ideal pattern which best describes the dataset, how do you optimize the model to do so",
            "Chain Rule": "basis of backpropagation, how neural networks improve themselves"
        }
    }
}
