![w700](../images/activation-functions/all.png)

- Activation Functions are used to introduce non-linearity in the network. 
- A neural network will almost always have the same activation function in all hidden layers. This activation function should be differentiable so that the parameters of the network are learned in backpropagation. 
- `ReLU` is the most commonly used activation function for hidden layers. 
- While selecting an activation function, you must consider the problems it might face: `vanishing` and `exploding` gradients. 
- Regarding the output layer, we must always consider the expected value range of the predictions. If it can be any numeric value (as in case of the regression problem) you can use the linear activation function or `ReLU`.

---

### Threshold [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Threshold.html#torch.nn.Threshold)
> AKA: Binary Step Function

![w700](../images/activation-functions/binary-threshold.png)

```sh
f(x) = {
    1:  if x >= threshold
    0:  if x < threshold
}
```

> PyTorch implementation
```
f(x) = {
        x:  if x > threshold
    value:  otherwise
}
```

- This type of activation function is rarely used in hidden layers because it is `not differentiable`, which makes it challenging to use with gradient-based optimization algorithms like gradient descent. Instead, more continuous and differentiable activation functions like `sigmoid`, `tanh`, or `ReLU` are commonly employed.
- However, threshold functions can be used in output layers for binary classification problems, where the network needs to make a binary decision based on the input data.

---

### ReLU [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)
> Rectified Linear Unit

![w700](../images/activation-functions/relu.png)

```sh
f(x) = max(0, x)
```

- Most widely used activation functions in neural networks.
- Known for its simplicity and effectiveness.
- Introduces non-linearity to the model, allowing it to learn complex relationships in the data.
- Computationally efficient because it involves simple operations
- Can lead to sparse activations because it sets negative values to 0. Sparse activations can be advantageous in some cases.

> Disadvantages

- Neurons with `ReLU` activations can sometimes become `"dead"` during training, meaning they always output 0 and don't contribute to learning. This can happen when a large gradient flows through a `ReLU` neuron, causing it to update in a way that always produces 0. This lead to the development of variants like `Exponential Linear Unit (ELU)`
- `ReLU` has an unbounded output range for positive values, which can be problematic in some cases. This issue led to the development of variants like `Leaky ReLU`, `Parametric ReLU (PReLU)`

> Common problems in using ReLU

- `Vanishing Gradients`: One common issue with ReLU is that during training, gradients can become very small, effectively "vanishing." This often happens when the network has a deep architecture. If you notice that your network is struggling to learn, and gradients are becoming extremely small, it could be a sign of a dying ReLU problem.
- `Neurons Stuck at Zero`: Some ReLU neurons may become "dead" during training, meaning they always output zero. You can detect this by monitoring the activation values of neurons during training. If you see that many neurons consistently output zero, it's a sign of the dying ReLU problem.
- `Unstable Training`: If your model's training is unstable, with erratic changes in loss or accuracy, it could be related to the ReLU activation function. ReLU can sometimes cause abrupt changes in neuron activations, leading to instability.
- `Poor Convergence`: If your network is converging slowly or not converging at all to a satisfactory solution, it might be due to ReLU-related issues. Poor convergence can result from neurons with zero activations not contributing to learning.

---

### Leaky ReLU [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)
> Leaky Rectified Linear Unit

![w700](../images/activation-functions/leaky-relu.png)

- A `ReLU` variant
- Addresses one of its limitation the `dying ReLU` problem
- Leaky ReLU introduces a small, non-zero gradient for negative inputs, preventing neurons from becoming completely inactive.

```sh
f(x) = {
        x  : if x > 0
    n * x  : if x <= 0
}

# n is negative slope
```

> Pytorch version
```sh
f(x) = max(0, x) + n * min(0, x)

# n is negative slope
```

- `Non-Linearity`: Like `ReLU`, `Leaky ReLU` introduces non-linearity to the network, which is crucial for learning complex relationships in the data.
- `Prevents Dying Neurons`: By allowing a non-zero gradient for negative inputs, `Leaky ReLU` helps prevent the `dying ReLU` problem, where neurons become inactive during training.
- `Computational Efficiency`: `Leaky ReLU` is computationally efficient to compute because it involves simple operations (multiplication and comparison).
- `Variants`: There are variations of `Leaky ReLU`, such as `Parametric ReLU (PReLU)`, where negative_slope is a learnable parameter, allowing the network to adaptively determine the slope for each neuron.

---

### PReLU [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#torch.nn.PReLU)
> Parametric Rectified Linear Unit

![w700](../images/activation-functions/prelu.png)

- A `ReLU` variant
- `PReLU` introduces learnable parameters to control the slope of the activation function, making it adaptable during training.

```sh
f(x) = {
    x if x > 0
    Î± * x if x <= 0
}

# Î± is alpha
```

- alphas is the learnable parameter

> Advantages

- `Adaptive Slope`: `PReLU` allows each neuron to have its own learnable slope parameter `alpha`. This adaptability helps improve model performance and mitigates issues like the `dying ReLU` problem.
- `Non-Linearity`: Like `ReLU` and `Leaky ReLU`, `PReLU` introduces non-linearity to the network, which is crucial for learning complex relationships in the data.
- `Prevents Dying Neurons`: The inclusion of a learnable parameter alpha prevents neurons from becoming completely inactive during training.
- `Variability`: `PReLU` introduces variability in the slope of the activation function, which can be beneficial in modeling complex data distributions.

---

### RReLU [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html#torch.nn.RReLU)
> Randomized Leaky Rectified Linear Unit

- A `ReLU` variant

![w700](../images/activation-functions/rrelu.png)
![w350](../images/activation-functions/rrelu-pic-2.png)

- `RReLU` introduces randomization to the slope of the activation function during training.

```sh
f(x) = {
    x if x > 0
    Î± * x if x <= 0
}

# Î± is alpha
```

- alpha is randomly picked within predifined range [lower, upper]

> Advantages

- `Randomization`: `RReLU` introduces randomness into the activation function, which can act as a form of `regularization`. This can help prevent `overfitting` and improve the `generalization` of the model.
- `Leaky Behavior`: Similar to `Leaky ReLU`, `RReLU` prevents neurons from becoming completely inactive during training, mitigating the `dying ReLU` problem.
- `Adaptability`: `RReLU` adapts the slope of the activation function for each neuron independently during training, potentially making it more flexible in capturing complex patterns in the data.
- `Controlled Randomness`: The randomization in `RReLU` is controlled by specifying the [lower, upper] range, allowing you to adjust the level of randomization.

---

### ReLU6 [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.ReLU6.html#torch.nn.ReLU6)
> Leaky Rectified Linear Unit 6

![w700](../images/activation-functions/relu-6.png)

- A `ReLU` variant
- `ReLU6` introduces an upper threshold value of 6 to the standard ReLU, which helps bound the output within a limited range.
- Clamps value between 0 and 6

```sh
f(x) = min(max(0, x), 6)
```

- `Bounded Activation`: `ReLU6` bounds the positive side of the activation output to a maximum value of 6, preventing activations from growing too large during training. This can help with model stability.
- `Non-linearity`: Like `ReLU`, `ReLU6` introduces non-linearity to the network, which is crucial for learning complex relationships in the data.
- `Efficiency`: `ReLU6` is computationally efficient to compute because it involves simple comparisons and thresholding.
- `Saturating Behavior`: `ReLU6` saturates for inputs greater than or equal to 6, which can help stabilize training.
- `Suitable for Mobile Devices`: `ReLU6` is often used in models designed for mobile and embedded devices due to its efficiency and bounded output range.

---

### Tanh [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh)
> Hyperbolic Tangent

![w700](../images/activation-functions/tanh.png)

- `exp(x)` is e<sup>x</sup>, where e is eulers number, e approx `2.71828`
- `tanh` output range is `-1` to `1`

```sh
f(x) = ( exp(x) âˆ’ exp(âˆ’x) ) / ( exp(x) + exp(-x) )
```

- `Range`: The output of `tanh` is bounded between `-1` and `1`. This makes it useful for problems where you want to normalize data or constrain the output within a specific range.
- As `x` becomes `very negative`, the `tanh` function approaches `-1`, and as `x` becomes `very positive`, it approaches `1`.
- `Symmetry`: The tanh function is symmetric around the origin `(0, 0)`. In other words, `tanh(-x) = -tanh(x)`.
- `S-shaped curve`: The tanh function has an S-shaped curve similar to the `sigmoid` function, but it ranges from `-1 to 1`, whereas the `sigmoid` function ranges from `0 to 1`.
- `Zero-Centered`: Unlike the `sigmoid` function, the `tanh` function is `zero-centered`, which means it has an average output of zero when its input is centered around zero. This property can help mitigate the vanishing gradient problem in some cases and make optimization easier.
- `Tanh` is often used in `recurrent neural networks (RNNs)` and in some `hidden layers` of `feedforward neural networks`, particularly when the input data or model requires `zero-centered` activations.
- However, for many deep learning applications, the `ReLU` and its variants have become more popular due to their simplicity and effectiveness in mitigating the `vanishing gradient` problem.

> Disadvantages

- `Saturation`: Like the `sigmoid`, the `tanh` function saturates when its input values are extremely positive or negative, causing the gradient to become very small. This can hinder the training process, particularly for deep networks.
- `Vanishing Gradient`: While the `tanh` function is `zero-centered`, it still suffers from the vanishing gradient problem, especially for very large or very small input values. In deep networks, this can slow down training.
- `Limited Output Range`: The output of the `tanh` function is limited to the range `[-1, 1]`, which can be a disadvantage in certain cases. For instance, in some applications where the model needs to produce outputs outside of this range, other activation functions like `ReLU` may be more appropriate.

---

### Hard tanh [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Hardtanh.html#torch.nn.Hardtanh)

```sh
f(x) = {
    -1 : if x < -1
     x  : if -1 <= x <= 1
     1  : if x > 1
}
```

```py
f(x) = min(-1, max(1, x))
```

![w700](../images/activation-functions/hard-tanh.png)

![w700](../images/activation-functions/hard-tanh-vs-tanh.png)

The `hard tanh` activation function has advantages such as mitigating exploding gradients, providing a bounded output range, and maintaining the non-linearity of the `tanh` function. However, it may suffer from the vanishing gradient problem and could introduce discontinuities in the network.

- `Gradient Stability`: One primary advantage of the `hard tanh` activation function is its ability to mitigate exploding gradients. By clipping the output to a fixed range, it helps in preventing excessively large gradient values during backpropagation, which can stabilize the training process in deep neural networks.

- `Bounded Output Range`: Similar to the standard tanh function, the `hard tanh` function produces output values within a bounded range [-1, 1]. This characteristic can be beneficial in scenarios where network activations need to be limited to a specific range, preventing extreme values.

- `Non-Linearity Preservation`: Despite the clipping, the `hard tanh` function still maintains the non-linear characteristics of the tanh function within the specified range. This non-linearity is crucial for enabling neural networks to learn complex relationships in data.

> Disadvantags

- `Saturation issues`: Like the standard `tanh` function, `hard tanh` can suffer from saturation in the tails of the function. This means that for extreme input values, the derivative becomes very small, leading to the vanishing gradient problem. This can hinder the learning process, especially in deep neural networks

- `Limited Output Range`: The `hard tanh` function limits its output to the range [-1, 1], unlike the traditional `tanh` function that outputs values in the range (-1, 1). This reduction in output range may affect the expressiveness of the network and limit its ability to model complex relationships in the data.

- `Non-Differentiability at Boundaries`: The `hard tanh` function introduces non-differentiability at its boundaries (-1 and 1). This can cause issues during backpropagation, as gradient-based optimization algorithms rely on the availability of derivatives. Discontinuities in the function can lead to challenges in training and convergence.

- `Non Zero-Centered`: The `hard tanh` function is not zero-centered, which can impact the convergence of certain optimization algorithms. Zero-centered activation functions are often preferred in neural networks to help the optimization process.

- `Lack of Flexibility`: The `hard tanh` function may not be as flexible as some other activation functions. The hardness of the saturation limits its ability to adapt and model the intricacies of complex data patterns.

---

### Tanh shrink [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html#torch.nn.Tanhshrink)

```py
f(x) = x - tanh(x)
```

![w700](../images/activation-functions/tanh-shrink.png)

![w700](../images/activation-functions/tanh-shrink-vs-tanh.png)

- `Zero-Centered Output`: Like `tanh` activation function, `Tanhshrink` produces outputs centered around zero. This can help in weight initialization and learning in neural networks.

- `Mitigates Saturation Effects`: It subtracts the `tanh` from the input, effectively mitigating the saturation issues associated with regular `tanh` activation. Saturation can lead to vanishing gradients, and using `Tanhshrink` may help address this problem.

- `Non-linear Mapping`: It introduces non-linearity to the network, allowing the model to learn complex relationships in the data.

> Disadvantages

- `Limited Use Cases`: `Tanhshrink` may not always outperform other activation functions like `ReLU` or its variants. Its effectiveness depends on the specific characteristics of the data and the problem at hand

- `Potential Vanishing Gradient`: While `Tanhshrink` helps mitigate saturation effects, it doesn't completely eliminate the possibility of vanishing gradients. In deep networks, gradients can still become very small during backpropagation, impacting the training process.

- `Computational Cost`: The `Tanhshrink` function involves additional computation compared to simpler activation functions like `ReLU`. This can increase the overall computational cost, especially in large networks.

---

### Hardshrink [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Hardshrink.html#torch.nn.Hardshrink)

```sh
f(x) = {
    x : if x > Î»
    x : if x < -Î»
    0 : otherwise
}

# `Î»` is lambda (0.5 in pytorch)
```

![w700](../images/activation-functions/hardshrink.png)

- `Non-linearity`: Like many activation functions, `hardshrink` introduces non-linearity to the network. This non-linearity is crucial for enabling the network to learn complex patterns and relationships in the data.

- `Sparsity`: `hardshrink` introduces sparsity in the network. By setting values below a certain threshold to zero, it encourages sparse representations, which can be beneficial in reducing the computational load and improving generalization.

- `Simplicity`: It is a relatively simple function, making it computationally efficient. The simplicity of the function can be advantageous in certain scenarios, especially when dealing with lightweight models or real-time applications.

> Disadvantages

- `Vanishing Gradient Problem`: Like other thresholding activation functions, hardshrink can suffer from the vanishing gradient problem. In regions where the output is constant (either zero or the original value), the gradient becomes zero, making it challenging for the network to learn.

- `Lack of Smoothness`: The hardshrink function is not differentiable at the threshold points. This lack of smoothness can make optimization more challenging, especially when using gradient-based optimization algorithms.

- `Limited Expressiveness`: The hardshrink function might not be as expressive as some other activation functions like ReLU or variants of it. More expressive activation functions can sometimes lead to better learning of complex patterns in the data.

---

### Soft shrink [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html#torch.nn.Softshrink)

```sh
f(x) = {
    x - Î»  : if x > Î»
    x + Î»  : if x < -Î»
        0  : otherwise
}

# `Î»` is lambda (0.5 in pytorch)
```

![w700](../images/activation-functions/softshrink.png)

![w700](../images/activation-functions/softshrink-vs-hardshrink.png)

- `Smoothness and Differentiability`: The softshrink function is smooth and differentiable everywhere, including at the point where it transitions from the linear region to the zero region. This makes it suitable for gradient-based optimization algorithms commonly used in training neural networks.

- `Sparsity Induction`: Like other shrinkage functions, softshrink can induce sparsity in the network by pushing some activations toward zero. This can be beneficial in scenarios where a sparse representation is desired, potentially leading to more efficient storage and computation.

- `Control over Shrinkage`: The Î» parameter in the softshrink function allows for control over the amount of shrinkage applied. This flexibility enables users to adjust the sparsity level according to the specific needs of the model.

> Disadvantages

- `Vanishing Gradients`: The softshrink function, like other shrinkage functions, can suffer from vanishing gradients when the input values are small. In the zero region, the gradient becomes zero, and during backpropagation, this can lead to the model having difficulty learning from certain examples.

- `Sensitivity to Hyperparameter`: The performance of softshrink is sensitive to the choice of the Î» hyperparameter. Selecting an inappropriate value for Î» might result in either too much or too little shrinkage, impacting the model's performance.

- `Limited Non-linearity`: Softshrink is a relatively simple non-linear function. In cases where more complex non-linearities are required, other activation functions like ReLU or variants might be more suitable.

- `Not Suitable for All Architectures`: Softshrink might not be the best choice for all types of neural network architectures or tasks. Its effectiveness depends on the specific characteristics of the problem being solved.

---

### Sigmoid [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)

![w700](../images/activation-functions/sigmoid.png)

- output ranges between `0` and `1`
- As `x` becomes `very negative`, the `sigmoid` function approaches `0`, and as `x` becomes `very positive`, it approaches `1`.

```sh
f(x) = 1 / ( 1 + e^(-x) )
```

- `Sigmoid` squashe x into the range between `0` and `1`.
- `Sigmoid` is often used in the hidden layers to introduce non-linearity to the model.
- `Sigmoid` function was more popular in the past, but has been largely replaced by other activation functions like `ReLU`

> Disadvantages

- `Vanishing gradient` problem
- `Not Zero-Centered`: means the output is always positive, This can lead to issues in backpropagation, and can result in slower convergence during training.
- `Prone to Saturation`: The `Sigmoid` function is prone to saturation, meaning that for very large positive or negative inputs, the function's output becomes close to 1 or 0, respectively. In this regime, the gradient is very close to zero, leading to slow learning.
- `Biased Outputs`: `Sigmoid` outputs are biased towards extreme values (0 or 1) for inputs that are not near 0, making it less ideal for tasks where the model should be more uncertain about the prediction.

---

### Hard Sigmoid [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html#torch.nn.Hardsigmoid)

```sh
f(x) = max(0, min(1, (x + 1) / 2))
```

> Advantages

- `Computational Efficiency`: The hard sigmoid function involves simple arithmetic operations and comparisons, making it computationally more efficient than the standard sigmoid, especially in scenarios where faster computation is crucial.
- `Reduced Vanishing Gradient`: While not as zero-centered as the tanh function, the hard sigmoid's range is [0, 1], which can help mitigate the vanishing gradient problem compared to the standard sigmoid, especially in certain architectures.
- `Simplicity`: The hard sigmoid is a relatively straightforward function, and its simplicity can be an advantage in some cases.

> Disadvantages

- `Less Smooth`: The hard sigmoid is a piecewise linear approximation, and it lacks the smoothness of the standard sigmoid. This lack of smoothness can be a disadvantage in some contexts, particularly when smoothness is desired for optimization.
- `Limited Expressiveness`: The hard sigmoid may have limited expressiveness compared to more complex activation functions like the standard sigmoid or the Rectified Linear Unit (ReLU). This can affect the ability of the network to model complex relationships in data.

---

### Log Sigmoid [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.LogSigmoid.html#torch.nn.LogSigmoid)

---

### Softsign [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Softsign.html#torch.nn.Softsign)

---

### Softplus [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html#torch.nn.Softplus)

---

### Softmin [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Softmin.html#torch.nn.Softmin)

---

### Softmax [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)

---

### Log Softmax [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)

---

### Elu [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU)

---

### Selu [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html#torch.nn.SELU)

---

### Celu [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.CELU.html#torch.nn.CELU)

---

### Glu [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.GLU.html#torch.nn.GLU)

---

### Gelu [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU)

---

### Silu [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html#torch.nn.SiLU)

---

### Hard swish [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Hardswish.html#torch.nn.Hardswish)

---

### Mish [ðŸ”—](https://pytorch.org/docs/stable/generated/torch.nn.Mish.html#torch.nn.Mish)

---


## Choosing the right Activation Function
1. Start with `ReLU`, move to others if not giving optimum results
1. Try variants of `ReLU` like `Leaky ReLU`, `PReLU`, `RReLU`
1. `ReLU` activation function should only be used in the hidden layers.
1. `Sigmoid` / `Logistic` and `Tanh` functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to `vanishing gradients`).
1. `Swish` function is used in neural networks having a depth greater than 40 layers.

> For different types of training

1. `Regression` - `Linear`
1. `Binary Classification` - `Sigmoid` / `Logistic`
1. `Multiclass Classification` - `Softmax`
1. `Multilabel Classification` - `Sigmoid`

> For different types of networks

1. `Convolutional Neural Network (CNN)` - `ReLU`
1. `Recurrent Neural Network (RNN)` - `Tanh` and/or `Sigmoid`

